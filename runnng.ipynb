{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b91159-c4c5-4cde-b5a7-f1b0ef9040be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States for SonicTheHedgehog-Genesis: ['GreenHillZone.Act1', 'GreenHillZone.Act2', 'GreenHillZone.Act3', 'LabyrinthZone.Act1', 'LabyrinthZone.Act2', 'LabyrinthZone.Act3', 'MarbleZone.Act1', 'MarbleZone.Act2', 'MarbleZone.Act3', 'ScrapBrainZone.Act1', 'ScrapBrainZone.Act2', 'SpringYardZone.Act1', 'SpringYardZone.Act2', 'SpringYardZone.Act3', 'StarLightZone.Act1', 'StarLightZone.Act2', 'StarLightZone.Act3']\n",
      "Loading existent model: ./model-sonic/best_model_550000\n",
      "video_prefix: best_model_550000\n",
      "old rings\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import retro\n",
    "import random\n",
    "import time\n",
    "\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3 import A2C, PPO\n",
    "# from sbx import DDPG, DQN, PPO, SAC, TD3, TQC, CrossQ\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecNormalize, VecTransposeImage\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gymnasium.wrappers import FrameStackObservation\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.atari_wrappers import ClipRewardEnv, WarpFrame\n",
    "from pathlib import Path\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "LOAD_FROM=\"./model-sonic\"\n",
    "#LOAD_FROM=\"./model-mk3-lstm\"\n",
    "# LOAD_FROM=\"./model-stf2-features\"\n",
    "GAME=\"SonicTheHedgehog-Genesis\"\n",
    "STATE=\"GreenHillZone.Act1\"\n",
    "# STATE=\"Champion.Level1.RyuVsGuile\"\n",
    "\n",
    "states = retro.data.list_states(GAME)\n",
    "\n",
    "# print(retro.data.list_games())\n",
    "print(f\"States for {GAME}: {states}\")\n",
    "\n",
    "MAX_EPISODE=5000\n",
    "SAVE_VIDEO = True\n",
    "IS_RECURRENT = False\n",
    "USE_CURRICULUM = False\n",
    "DETERMINISTIC = True\n",
    "LOAD_FROM_VIDEO= LOAD_FROM + \"/best_model_550000\"\n",
    "\n",
    "# LOAD_FROM=\"./model-sbx\"\n",
    "# LOAD_FROM=\"./model2\"\n",
    "# GAME=\"SuperMarioBros-Nes\"\n",
    "# STATE=\"Level1-1\"\n",
    "\n",
    "class IgnorePauseActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, act):\n",
    "        act[3] = 0\n",
    "        return act\n",
    "\n",
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym environment and make it use discrete actions.\n",
    "\n",
    "    Args:\n",
    "        combos: ordered list of lists of valid button combinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    # def action(self, act):\n",
    "    #     return self._decode_discrete_action[act].copy()\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act]\n",
    "\n",
    "class MainDiscretizer(Discretizer):\n",
    "    \"\"\"\n",
    "    Use Sonic-specific discrete actions\n",
    "    based on https://github.com/openai/retro-baselines/blob/master/agents/sonic_util.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(\n",
    "            env=env,\n",
    "            combos=[\n",
    "                [\"LEFT\"],\n",
    "                [\"RIGHT\"],\n",
    "                [\"B\"],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class ResetStateWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "        self.steps = 0\n",
    "        self.x_last = 0\n",
    "        self.lose_lives = False\n",
    "        self.old_rings = 0\n",
    "        #self.current_health = 40\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        #self.x_last = self.env.unwrapped.data['x']\n",
    "\n",
    "        self.steps = 0\n",
    "        self.old_rings = 0\n",
    "\n",
    "        #self.current_health = 40\n",
    "\n",
    "        self.lose_lives = False\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, trunc, info = self.env.step(action)\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        # reward = 0\n",
    "        if reward > 0:\n",
    "            reward = 0.1 # new score reward\n",
    "\n",
    "        # Penalty for excessive jumping\n",
    "        if action == 2:\n",
    "            reward -= 0.08\n",
    "        \n",
    "        # Existential\n",
    "        reward -= 0.05\n",
    "\n",
    "        max_x = info['screen_x_end']\n",
    "        current_x = info['x']\n",
    "        current_rings = info['rings']\n",
    "\n",
    "        if current_x > self.x_last:\n",
    "            reward += (current_x / max_x) * 0.5\n",
    "        elif current_x <= self.x_last:\n",
    "            reward -= 0.02\n",
    "\n",
    "        if reward > 0.9:\n",
    "            print(reward)\n",
    "\n",
    "        if self.x_last > 0:\n",
    "            velocity = current_x - self.x_last\n",
    "\n",
    "            if velocity > 4 :\n",
    "                reward += 0.1\n",
    "            # elif velocity == 0:\n",
    "            #     print(\"stopped\")\n",
    "            \n",
    "        self.x_last = current_x\n",
    "\n",
    "        if self.steps > MAX_EPISODE:\n",
    "            done = True\n",
    "            reward -= 1\n",
    "\n",
    "        # Sonic collide with obstacle\n",
    "        if current_rings < self.old_rings:\n",
    "            print(\"old rings\")\n",
    "\n",
    "        self.old_rings = current_rings\n",
    "\n",
    "        if current_x >= max_x:\n",
    "            print(current_x)\n",
    "            reward += 2\n",
    "            done = True\n",
    "\n",
    "        if info['lives'] < 3:\n",
    "            reward -= 1\n",
    "            done = True\n",
    "\n",
    "        \n",
    "\n",
    "        # if self.current_health != info['health'] and info['health'] < self.current_health:\n",
    "        #     reward += (info['health'] - 40) / 40 * 2\n",
    "        #     self.current_health = info['health']\n",
    "\n",
    "        # if info['lives'] < 3 or info['health'] <= 0:\n",
    "        #     reward -= 1\n",
    "        #     done = True \n",
    "        #     self.lose_lives = True\n",
    "\n",
    "        # if done and not self.lose_lives:\n",
    "        #     reward += 2\n",
    " \n",
    "        return obs, reward, done, trunc, info\n",
    "\n",
    "\n",
    "class RandomStateWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "\n",
    "    def get_random_state(self):\n",
    "        \"\"\"Select a random state from folder STATE_PATH\"\"\"\n",
    "        STATE_PATH = \"./States\"\n",
    "        states = [f for f in os.listdir(STATE_PATH) if f.endswith(\".state\")]\n",
    "        if not states:\n",
    "            raise FileNotFoundError(\"File not found!\")\n",
    "        c = random.choice(states)\n",
    "\n",
    "        return os.path.abspath(\"./States/\" + c)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        state = self.get_random_state()\n",
    "        print(f\"Loading state: {state}\")\n",
    "        self.env.load_state(state)\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, trunc, info = self.env.step(action)\n",
    "      \n",
    "    \n",
    "        return obs, reward, done, trunc, info\n",
    "\n",
    "\n",
    "class CurriculumWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, required_wins=20, required_avg_reward=1.0):\n",
    "        super().__init__(env)\n",
    "        self.required_wins = required_wins\n",
    "        self.required_avg_reward = required_avg_reward\n",
    "        self.current_phase = 1\n",
    "        self.total_wins = 0 \n",
    "        self.rewards_list = []\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "        self.rewards_list.append(reward)\n",
    "\n",
    "        could_to_next_stage = info[\"matches_won\"] / self.current_phase >= 2\n",
    "\n",
    "        if info[\"matches_won\"] % 2 == 0 and info[\"matches_won\"] > 0 and could_to_next_stage:\n",
    "            self.total_wins += 1\n",
    "\n",
    "\n",
    "        avg_reward = np.mean(self.rewards_list[-self.required_wins:]) if len(self.rewards_list) >= self.required_wins else np.mean(self.rewards_list)\n",
    "\n",
    "        if could_to_next_stage and \\\n",
    "            ((info[\"matches_won\"] % 2 == 0  and info[\"matches_won\"] > 0) \\\n",
    "                 or (info[\"enemy_matches_won\"] % 2 == 0 and info[\"enemy_matches_won\"] > 0)) :\n",
    "            print(info)\n",
    "            print(f\"ðŸ”¥ stage {self.current_phase}! ({self.total_wins} fights win, avg rewards: {avg_reward:.2f})\")\n",
    "            done = True\n",
    "        \n",
    "        if self.total_wins >= self.required_wins and avg_reward >= self.required_avg_reward:\n",
    "            self.current_phase += 1\n",
    "            print(f\"ðŸ”¥ Going to next stage {self.current_phase}! ({self.total_wins} fights win, avg rewards: {avg_reward:.2f})\")\n",
    "            self.total_wins = 0\n",
    "            self.rewards_list = []\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "def make_test_env(video_prefix):\n",
    "    env = retro.make(\n",
    "        game=GAME, \n",
    "        render_mode=\"rgb_array\", # rgb_array or human\n",
    "        state=STATE,\n",
    "        #use_restricted_actions=retro.Actions.DISCRETE,\n",
    "    )\n",
    "    # env = RandomStateWrapper(env)\n",
    "    # env = IgnorePauseActionWrapper(env)\n",
    "    # env = DonkeyKongCustomActions(env)\n",
    "    env = MainDiscretizer(env)\n",
    "    env = ResetStateWrapper(env)\n",
    "    env = WarpFrame(env)\n",
    "    #env = ClipRewardEnv(env)\n",
    "\n",
    "    if USE_CURRICULUM:\n",
    "        env = CurriculumWrapper(env, required_wins=3, required_avg_reward=0.3)\n",
    "\n",
    "    if video_prefix != None:\n",
    "        video_prefix = video_prefix.replace(LOAD_FROM, \"\").replace(\"/\", \"\")\n",
    "\n",
    "    print(\"video_prefix:\", video_prefix)\n",
    "    \n",
    "    if SAVE_VIDEO:\n",
    "        env = RecordVideo(\n",
    "            env, \n",
    "            video_folder=\"videos/\", \n",
    "            # episode_trigger=lambda e: True, \n",
    "            episode_trigger=lambda episode_id: episode_id == 0, # record only first episode\n",
    "            fps=60,\n",
    "            name_prefix=f\"gameplay_{video_prefix}\"\n",
    "        )\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    # env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "    env = VecFrameStack(env, 4)\n",
    "    #env = VecTransposeImage(env)\n",
    "    # env = TimeLimit(env, max_episode_steps=MAX_EPISODE)\n",
    "    return env\n",
    "\n",
    "save_dir = Path(LOAD_FROM)\n",
    "\n",
    "\n",
    "def get_latest_model(path):\n",
    "    models = list(path.glob(\"best_model_*\"))\n",
    "    if not models:\n",
    "        return None\n",
    "    model_numbers = [int(re.search(r\"best_model_(\\d+)\", str(m)).group(1)) for m in models]\n",
    "    latest_model = max(model_numbers)\n",
    "    return path / f\"best_model_{latest_model}\"\n",
    "\n",
    "latest_model_path = get_latest_model(save_dir)\n",
    "\n",
    "if SAVE_VIDEO:\n",
    "    latest_model_path = LOAD_FROM_VIDEO\n",
    "\n",
    "\n",
    "print(f\"Loading existent model: {latest_model_path}\")\n",
    "\n",
    "# latest_model_path= LOAD_FROM + \"/best_model_413000\"\n",
    "\n",
    "env = make_test_env(f\"{latest_model_path}\")\n",
    "\n",
    "env.metadata['render_fps'] = 60\n",
    "\n",
    "model = None\n",
    "\n",
    "if IS_RECURRENT:\n",
    "    model = RecurrentPPO.load(str(latest_model_path), device=\"cuda\", verbose=0)\n",
    "else:\n",
    "    # model = PPO.load(str(latest_model_path), device=\"cuda\", verbose=0)\n",
    "    model = PPO.load(\n",
    "        str(latest_model_path), \n",
    "        env=env, \n",
    "        verbose=0, \n",
    "        # learning_rate=LEARNING_RATE,\n",
    "        # gae_lambda=GAE,\n",
    "        # clip_range=CLIP_RANGE,\n",
    "        # ent_coef=ENT_COEF,\n",
    "        # gamma=GAMMA,\n",
    "        # n_steps=N_STEPS\n",
    "    )\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "\n",
    "render_mode = \"human\"\n",
    "\n",
    "if SAVE_VIDEO:\n",
    "    render_mode = \"rgb_array\"\n",
    "\n",
    "# obs = env.reset()\n",
    "# while True:  # Testar cada botÃ£o individualmente\n",
    "#     env.render()\n",
    "#     # action = 0\n",
    "#     # action = i\n",
    "#     # print(f\"Testando botÃ£o no Ã­ndice {i}: {action}\")\n",
    "    \n",
    "    \n",
    "#     action = [1]\n",
    "\n",
    "#     print(action)\n",
    "\n",
    "#     #action = [[action]]\n",
    "\n",
    "\n",
    "#     # if step % 5 == 0:\n",
    "#     #     action = [ action[i] ]\n",
    "#     # else:\n",
    "#     #     action = [ action[0] ]\n",
    "\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     time.sleep(0.01)\n",
    "\n",
    "while True:\n",
    "    # env.render(\"human\") \n",
    "    env.render(render_mode) \n",
    "    action, _ = model.predict(obs, deterministic=DETERMINISTIC)\n",
    "\n",
    "    # print(action)\n",
    "    \n",
    "    #print(action)\n",
    "    #action = [0]\n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "    #print(info)\n",
    "\n",
    "    if done:\n",
    "        if SAVE_VIDEO:\n",
    "            break\n",
    "        else:\n",
    "            obs = env.reset()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478edcf-de94-4eda-98e2-85aa7d58dc58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
