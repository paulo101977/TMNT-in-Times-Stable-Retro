{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65b91159-c4c5-4cde-b5a7-f1b0ef9040be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States for TeenageMutantNinjaTurtlesIVTurtlesInTime-Snes: ['1Player.Leonardo.Level1', 'Leo.Level1.99Lives', 'Leo.Level1.InfinitiLives']\n",
      "Loading existent model: model-tmnt/best_model_7644000\n",
      "video_prefix: model-tmntbest_model_7644000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 359\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender(render_mode) \n\u001b[0;32m--> 359\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDETERMINISTIC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;66;03m# step_count+= 1\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \n\u001b[1;32m    366\u001b[0m     \u001b[38;5;66;03m# print(step_count)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/common/base_class.py:557\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    539\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    543\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, Optional[\u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    544\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/common/policies.py:352\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03mGet the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03mIncludes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m    (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# Switch to eval mode (this affects batch norm / dropout)\u001b[39;00m\n\u001b[0;32m--> 352\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_training_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/common/policies.py:211\u001b[0m, in \u001b[0;36mBaseModel.set_training_mode\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_training_mode\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    Put the policy in either training or evaluation mode.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    :param mode: if true, set to training mode, else set to evaluation mode\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:2843\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2842\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2843\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:2843\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2842\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2843\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:2843\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m   2842\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m-> 2843\u001b[0m     \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:2842\u001b[0m, in \u001b[0;36mModule.train\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m   2840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining mode is expected to be boolean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2841\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m-> 2842\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:2725\u001b[0m, in \u001b[0;36mModule.children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchildren\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   2720\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over immediate children modules.\u001b[39;00m\n\u001b[1;32m   2721\u001b[0m \n\u001b[1;32m   2722\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[1;32m   2723\u001b[0m \u001b[38;5;124;03m        Module: a child module\u001b[39;00m\n\u001b[1;32m   2724\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2725\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2726\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:2728\u001b[0m, in \u001b[0;36mModule.named_children\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2725\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m   2726\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m module\n\u001b[0;32m-> 2728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnamed_children\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[1;32m   2729\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\u001b[39;00m\n\u001b[1;32m   2730\u001b[0m \n\u001b[1;32m   2731\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2740\u001b[0m \n\u001b[1;32m   2741\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   2742\u001b[0m     memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import retro\n",
    "import random\n",
    "import time\n",
    "import sounddevice as sd\n",
    "\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3 import A2C, PPO\n",
    "# from sbx import DDPG, DQN, PPO, SAC, TD3, TQC, CrossQ\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecNormalize, VecTransposeImage\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gymnasium.wrappers import FrameStackObservation\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.atari_wrappers import ClipRewardEnv, WarpFrame\n",
    "from pathlib import Path\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "LOAD_FROM=\"./model-tmnt\"\n",
    "GAME=\"TeenageMutantNinjaTurtlesIVTurtlesInTime-Snes\"\n",
    "STATE=\"Leo.Level1.99Lives\"\n",
    "\n",
    "states = retro.data.list_states(GAME)\n",
    "\n",
    "# print(retro.data.list_games())\n",
    "print(f\"States for {GAME}: {states}\")\n",
    "\n",
    "MAX_EPISODE=122000\n",
    "SAVE_VIDEO = False\n",
    "IS_RECURRENT = False\n",
    "USE_CURRICULUM = False\n",
    "DETERMINISTIC = False\n",
    "LOAD_FROM_VIDEO= LOAD_FROM + \"/best_model_100000\"\n",
    "\n",
    "# LOAD_FROM=\"./model-sbx\"\n",
    "# LOAD_FROM=\"./model2\"\n",
    "# GAME=\"SuperMarioBros-Nes\"\n",
    "# STATE=\"Level1-1\"\n",
    "\n",
    "class IgnorePauseActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, act):\n",
    "        act[3] = 0\n",
    "        return act\n",
    "\n",
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym environment and make it use discrete actions.\n",
    "\n",
    "    Args:\n",
    "        combos: ordered list of lists of valid button combinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    # def action(self, act):\n",
    "    #     return self._decode_discrete_action[act].copy()\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act]\n",
    "\n",
    "class MainDiscretizer(Discretizer):\n",
    "    \"\"\"\n",
    "    Use Sonic-specific discrete actions\n",
    "    based on https://github.com/openai/retro-baselines/blob/master/agents/sonic_util.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(\n",
    "            env=env,\n",
    "            combos=[\n",
    "                [\"LEFT\"],         # Move left\n",
    "                [\"RIGHT\"],        # Move right\n",
    "                [\"UP\"],           # Move up\n",
    "                [\"DOWN\"],         # Move down\n",
    "                [\"B\"],            # Attack\n",
    "                [\"Y\"],            # Special attack\n",
    "                [\"A\"],            # Jump attack\n",
    "                [\"X\"],            # Throw (in some cases)\n",
    "                [\"L\"],            # Block (if available)\n",
    "                [\"R\"],            # Not used, but reserved\n",
    "                \n",
    "                # Combined actions\n",
    "                [\"LEFT\", \"B\"],     # Move left and attack\n",
    "                [\"RIGHT\", \"B\"],    # Move right and attack\n",
    "                [\"UP\", \"B\"],       # Move up and attack\n",
    "                [\"DOWN\", \"B\"],     # Move down and attack\n",
    "                \n",
    "                [\"LEFT\", \"A\"],     # Move left and jump\n",
    "                [\"RIGHT\", \"A\"],    # Move right and jump\n",
    "                [\"UP\", \"A\"],       # Move up and jump\n",
    "                [\"DOWN\", \"A\"],     # Move down and jump\n",
    "\n",
    "                [\"LEFT\", \"A\", \"B\"],  # Jump left and attack\n",
    "                [\"RIGHT\", \"A\", \"B\"], # Jump right and attack\n",
    "\n",
    "                # [\"B\", \"Y\"],        # Attack + Special\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class ResetStateWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "        self.steps = 0\n",
    "        self.lose_lives = False\n",
    "        self.current_health = 16\n",
    "        self.current_lives = 2\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        #self.x_last = self.env.unwrapped.data['x']\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "        self.current_health = 16\n",
    "\n",
    "        self.current_lives = 3\n",
    "\n",
    "        self.lose_lives = False\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, trunc, info = self.env.step(action)\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        # reward = 0\n",
    "        if reward > 0:\n",
    "            reward = 0.1 # new score reward\n",
    "\n",
    "        # Existential to encourage staying alive longer\n",
    "        reward -= 0.05\n",
    "\n",
    "        lives = info['lives']\n",
    "\n",
    "        health = info['health']\n",
    "\n",
    "        if health == 80:\n",
    "            health = 16\n",
    "\n",
    "        if health < self.current_health:\n",
    "            self.current_health = health\n",
    "            reward -=0.5\n",
    "\n",
    "        if MAX_EPISODE > 0 and self.steps > MAX_EPISODE:\n",
    "            done = True\n",
    "            reward -= 1\n",
    "\n",
    "        # if lives < 2:\n",
    "        #     reward -= 1\n",
    "        #     self.current_lives =  info['lives']\n",
    "        #     done = True\n",
    "\n",
    "        # if lives < 1:\n",
    "        #     done = True\n",
    "        \n",
    "        return obs, reward, done, trunc, info\n",
    "\n",
    "\n",
    "class RandomStateWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "\n",
    "    def get_random_state(self):\n",
    "        \"\"\"Select a random state from folder STATE_PATH\"\"\"\n",
    "        STATE_PATH = \"./States\"\n",
    "        states = [f for f in os.listdir(STATE_PATH) if f.endswith(\".state\")]\n",
    "        if not states:\n",
    "            raise FileNotFoundError(\"File not found!\")\n",
    "        c = random.choice(states)\n",
    "\n",
    "        return os.path.abspath(\"./States/\" + c)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        state = self.get_random_state()\n",
    "        print(f\"Loading state: {state}\")\n",
    "        self.env.load_state(state)\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, trunc, info = self.env.step(action)\n",
    "      \n",
    "    \n",
    "        return obs, reward, done, trunc, info\n",
    "\n",
    "\n",
    "class CurriculumWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, required_wins=20, required_avg_reward=1.0):\n",
    "        super().__init__(env)\n",
    "        self.required_wins = required_wins\n",
    "        self.required_avg_reward = required_avg_reward\n",
    "        self.current_phase = 1\n",
    "        self.total_wins = 0 \n",
    "        self.rewards_list = []\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "        self.rewards_list.append(reward)\n",
    "\n",
    "        could_to_next_stage = info[\"matches_won\"] / self.current_phase >= 2\n",
    "\n",
    "        if info[\"matches_won\"] % 2 == 0 and info[\"matches_won\"] > 0 and could_to_next_stage:\n",
    "            self.total_wins += 1\n",
    "\n",
    "\n",
    "        avg_reward = np.mean(self.rewards_list[-self.required_wins:]) if len(self.rewards_list) >= self.required_wins else np.mean(self.rewards_list)\n",
    "\n",
    "        if could_to_next_stage and \\\n",
    "            ((info[\"matches_won\"] % 2 == 0  and info[\"matches_won\"] > 0) \\\n",
    "                 or (info[\"enemy_matches_won\"] % 2 == 0 and info[\"enemy_matches_won\"] > 0)) :\n",
    "            print(info)\n",
    "            print(f\"🔥 stage {self.current_phase}! ({self.total_wins} fights win, avg rewards: {avg_reward:.2f})\")\n",
    "            done = True\n",
    "        \n",
    "        if self.total_wins >= self.required_wins and avg_reward >= self.required_avg_reward:\n",
    "            self.current_phase += 1\n",
    "            print(f\"🔥 Going to next stage {self.current_phase}! ({self.total_wins} fights win, avg rewards: {avg_reward:.2f})\")\n",
    "            self.total_wins = 0\n",
    "            self.rewards_list = []\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "def make_test_env(video_prefix):\n",
    "    env = retro.make(\n",
    "        game=GAME, \n",
    "        render_mode=\"rgb_array\", # rgb_array or human\n",
    "        state=STATE,\n",
    "        #use_restricted_actions=retro.Actions.DISCRETE,\n",
    "    )\n",
    "\n",
    "    my_env = env\n",
    "    # env = RandomStateWrapper(env)\n",
    "    # env = IgnorePauseActionWrapper(env)\n",
    "\n",
    "    env = MainDiscretizer(env)\n",
    "    env = ResetStateWrapper(env)\n",
    "    env = WarpFrame(env)\n",
    "    #env = ClipRewardEnv(env)\n",
    "\n",
    "    if USE_CURRICULUM:\n",
    "        env = CurriculumWrapper(env, required_wins=3, required_avg_reward=0.3)\n",
    "\n",
    "    if video_prefix != None:\n",
    "        video_prefix = video_prefix.replace(LOAD_FROM, \"\").replace(\"/\", \"\")\n",
    "\n",
    "    print(\"video_prefix:\", video_prefix)\n",
    "    \n",
    "    if SAVE_VIDEO:\n",
    "        env = RecordVideo(\n",
    "            env, \n",
    "            video_folder=\"videos/\", \n",
    "            # episode_trigger=lambda e: True, \n",
    "            episode_trigger=lambda episode_id: episode_id == 0, # record only first episode\n",
    "            fps=60,\n",
    "            name_prefix=f\"gameplay_{video_prefix}\"\n",
    "        )\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    # env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "    env = VecFrameStack(env, 4)\n",
    "    #env = VecTransposeImage(env)\n",
    "    # env = TimeLimit(env, max_episode_steps=MAX_EPISODE)\n",
    "    return env, my_env\n",
    "\n",
    "save_dir = Path(LOAD_FROM)\n",
    "\n",
    "\n",
    "def get_latest_model(path):\n",
    "    models = list(path.glob(\"best_model_*\"))\n",
    "    if not models:\n",
    "        return None\n",
    "    model_numbers = [int(re.search(r\"best_model_(\\d+)\", str(m)).group(1)) for m in models]\n",
    "    latest_model = max(model_numbers)\n",
    "    return path / f\"best_model_{latest_model}\"\n",
    "\n",
    "latest_model_path = get_latest_model(save_dir)\n",
    "\n",
    "\n",
    "if SAVE_VIDEO:\n",
    "    latest_model_path = LOAD_FROM_VIDEO\n",
    "\n",
    "\n",
    "print(f\"Loading existent model: {latest_model_path}\")\n",
    "\n",
    "# latest_model_path= LOAD_FROM + \"/best_model_413000\"\n",
    "\n",
    "env, my_env = make_test_env(f\"{latest_model_path}\")\n",
    "\n",
    "env.metadata['render_fps'] = 60\n",
    "\n",
    "model = None\n",
    "\n",
    "if IS_RECURRENT:\n",
    "    model = RecurrentPPO.load(str(latest_model_path), device=\"cuda\", verbose=0)\n",
    "else:\n",
    "    # model = PPO.load(str(latest_model_path), device=\"cuda\", verbose=0)\n",
    "    model = PPO.load(\n",
    "        str(latest_model_path), \n",
    "        env=env, \n",
    "        verbose=0, \n",
    "        # learning_rate=LEARNING_RATE,\n",
    "        # gae_lambda=GAE,\n",
    "        # clip_range=CLIP_RANGE,\n",
    "        # ent_coef=ENT_COEF,\n",
    "        # gamma=GAMMA,\n",
    "        # n_steps=N_STEPS\n",
    "    )\n",
    "\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "\n",
    "render_mode = \"human\"\n",
    "\n",
    "if SAVE_VIDEO:\n",
    "    render_mode = \"rgb_array\" \n",
    "\n",
    "step_count = 0\n",
    "while True:\n",
    "    env.render(render_mode) \n",
    "    action, _ = model.predict(obs, deterministic=DETERMINISTIC)\n",
    "    \n",
    "   \n",
    "    obs, reward, done, info = env.step(action)\n",
    "\n",
    "    # step_count+= 1\n",
    "\n",
    "    # print(step_count)\n",
    "\n",
    "    if done:\n",
    "        if SAVE_VIDEO:\n",
    "            break\n",
    "        else:\n",
    "            obs = env.reset()\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9151ae1-0737-4cb7-bcf0-b962c970d423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
