{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d34804-70a0-4608-8922-7b4a3cd105bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States for TeenageMutantNinjaTurtlesIVTurtlesInTime-Snes: ['1Player.Leonardo.Level1', 'Leo.Level1.99Lives', 'Leo.Level1.InfinitiLives']\n",
      "Loading existent model: model-tmnt/best_model_7368000\n",
      "Model saved in: model-tmnt/best_model_7369000\n",
      "Time steps: 1000, Average Reward: -0.19304999999999778, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7370000\n",
      "Time steps: 2000, Average Reward: -0.1981749999999933, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7371000\n",
      "Time steps: 3000, Average Reward: -0.20074999999999943, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7372000\n",
      "Time steps: 4000, Average Reward: -0.20191250000001004, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7373000\n",
      "Time steps: 5000, Average Reward: -0.20206000000001614, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7374000\n",
      "Time steps: 6000, Average Reward: -0.20160833333335457, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7375000\n",
      "Time steps: 7000, Average Reward: -0.201285714285739, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7376000\n",
      "Time steps: 8000, Average Reward: -0.2016750000000274, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7377000\n",
      "Time steps: 9000, Average Reward: -0.2022277777778073, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7378000\n",
      "Time steps: 10000, Average Reward: -0.20208000000003118, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7379000\n",
      "Time steps: 11000, Average Reward: -0.2020409090909241, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7380000\n",
      "Time steps: 12000, Average Reward: -0.2020291666666655, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7381000\n",
      "Time steps: 13000, Average Reward: -0.2019884615384467, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7382000\n",
      "Time steps: 14000, Average Reward: -0.2019392857142592, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7383000\n",
      "Time steps: 15000, Average Reward: -0.20183666666663, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7384000\n",
      "Time steps: 16000, Average Reward: -0.20174687499995464, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7385000\n",
      "Time steps: 17000, Average Reward: -0.20119999999994786, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7386000\n",
      "Time steps: 18000, Average Reward: -0.20143888888883, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7387000\n",
      "Time steps: 19000, Average Reward: -0.20155789473677696, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7388000\n",
      "Time steps: 20000, Average Reward: -0.20176999999992917, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7389000\n",
      "Time steps: 21000, Average Reward: -0.20203571428563857, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7390000\n",
      "Time steps: 22000, Average Reward: -0.20195681818173725, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7391000\n",
      "Time steps: 23000, Average Reward: -0.202060869565132, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7392000\n",
      "Time steps: 24000, Average Reward: -0.20203124999991054, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7393000\n",
      "Time steps: 25000, Average Reward: -0.20193999999990675, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7394000\n",
      "Time steps: 26000, Average Reward: -0.20183076923067253, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7395000\n",
      "Time steps: 27000, Average Reward: -0.20188888888878895, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7396000\n",
      "Time steps: 28000, Average Reward: -0.20187857142846852, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7397000\n",
      "Time steps: 29000, Average Reward: -0.20195517241368735, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7398000\n",
      "Time steps: 30000, Average Reward: -0.20192999999989167, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7399000\n",
      "Time steps: 31000, Average Reward: -0.19589178356713588, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7400000\n",
      "Time steps: 32000, Average Reward: -0.20300200133421764, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7401000\n",
      "Time steps: 33000, Average Reward: -0.20136054421767946, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7402000\n",
      "Time steps: 34000, Average Reward: -0.20195770220063505, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7403000\n",
      "Time steps: 35000, Average Reward: -0.20227828406313922, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7404000\n",
      "Time steps: 36000, Average Reward: -0.20288234224406398, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7405000\n",
      "Time steps: 37000, Average Reward: -0.20198492075706323, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7406000\n",
      "Time steps: 38000, Average Reward: -0.20184024536607548, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7407000\n",
      "Time steps: 39000, Average Reward: -0.20206494881753687, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7408000\n",
      "Time steps: 40000, Average Reward: -0.20191072744502497, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7409000\n",
      "Time steps: 41000, Average Reward: -0.2017620725783656, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7410000\n",
      "Time steps: 42000, Average Reward: -0.20166971040960804, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7411000\n",
      "Time steps: 43000, Average Reward: -0.2016641331306429, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7412000\n",
      "Time steps: 44000, Average Reward: -0.20168901400101708, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7413000\n",
      "Time steps: 45000, Average Reward: -0.20121387681906094, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7414000\n",
      "Time steps: 46000, Average Reward: -0.2013387960513189, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7415000\n",
      "Time steps: 47000, Average Reward: -0.2013940238801875, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7416000\n",
      "Time steps: 48000, Average Reward: -0.2011514943710526, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7417000\n",
      "Time steps: 49000, Average Reward: -0.20163792637433786, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7418000\n",
      "Time steps: 50000, Average Reward: -0.20151289809727174, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7419000\n",
      "Time steps: 51000, Average Reward: -0.2017488657982599, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7420000\n",
      "Time steps: 52000, Average Reward: -0.2016419368341943, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7421000\n",
      "Time steps: 53000, Average Reward: -0.20167785234891145, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7422000\n",
      "Time steps: 54000, Average Reward: -0.20177454359751362, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7423000\n",
      "Time steps: 55000, Average Reward: -0.20182048246858197, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7424000\n",
      "Time steps: 56000, Average Reward: -0.20162359308198763, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7425000\n",
      "Time steps: 57000, Average Reward: -0.20161515528878118, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7426000\n",
      "Time steps: 58000, Average Reward: -0.20178370122539824, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7427000\n",
      "Time steps: 59000, Average Reward: -0.20195270009463628, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7428000\n",
      "Time steps: 60000, Average Reward: -0.20225431370544267, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7429000\n",
      "Time steps: 61000, Average Reward: -0.2024295878552303, Best Reward: 0.8999999999999999\n",
      "Model saved in: model-tmnt/best_model_7430000\n",
      "Time steps: 62000, Average Reward: -0.1961422845691357, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7431000\n",
      "Time steps: 63000, Average Reward: -0.19717217217216526, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7432000\n",
      "Time steps: 64000, Average Reward: -0.19786524349566242, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7433000\n",
      "Time steps: 65000, Average Reward: -0.20031265632817333, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7434000\n",
      "Time steps: 66000, Average Reward: -0.20164065626252067, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7435000\n",
      "Time steps: 67000, Average Reward: -0.20083361120375526, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7436000\n",
      "Time steps: 68000, Average Reward: -0.20080022863677757, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7437000\n",
      "Time steps: 69000, Average Reward: -0.20105651412855907, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7438000\n",
      "Time steps: 70000, Average Reward: -0.20102800622363423, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7439000\n",
      "Time steps: 71000, Average Reward: -0.20109521904383948, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7440000\n",
      "Time steps: 72000, Average Reward: -0.20123204218950436, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7441000\n",
      "Time steps: 73000, Average Reward: -0.20148774795799246, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7442000\n",
      "Time steps: 74000, Average Reward: -0.20150792429603137, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7443000\n",
      "Time steps: 75000, Average Reward: -0.20137519645661103, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7444000\n",
      "Time steps: 76000, Average Reward: -0.20105680757430833, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7445000\n",
      "Time steps: 77000, Average Reward: -0.2013314164270096, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7446000\n",
      "Time steps: 78000, Average Reward: -0.2015943052123264, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7447000\n",
      "Time steps: 79000, Average Reward: -0.2017779753305337, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7448000\n",
      "Time steps: 80000, Average Reward: -0.20193178229280806, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7449000\n",
      "Time steps: 81000, Average Reward: -0.20185518551848192, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7450000\n",
      "Time steps: 82000, Average Reward: -0.20191208686534073, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7451000\n",
      "Time steps: 83000, Average Reward: -0.2020638239839186, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7452000\n",
      "Time steps: 84000, Average Reward: -0.20205235237838307, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7453000\n",
      "Time steps: 85000, Average Reward: -0.20220435036244144, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7454000\n",
      "Time steps: 86000, Average Reward: -0.20229618369460295, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7455000\n",
      "Time steps: 87000, Average Reward: -0.2023194091852258, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7456000\n",
      "Time steps: 88000, Average Reward: -0.20208348766565373, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7457000\n",
      "Time steps: 89000, Average Reward: -0.20210729337799577, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7458000\n",
      "Time steps: 90000, Average Reward: -0.20218635767973453, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7459000\n",
      "Time steps: 91000, Average Reward: -0.20226515100995923, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7460000\n",
      "Time steps: 92000, Average Reward: -0.1889336016096594, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7461000\n",
      "Time steps: 93000, Average Reward: -0.19832999331996817, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7462000\n",
      "Time steps: 94000, Average Reward: -0.1983980776932242, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7463000\n",
      "Time steps: 95000, Average Reward: -0.20191592793823823, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7464000\n",
      "Time steps: 96000, Average Reward: -0.20204580831666918, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7465000\n",
      "Time steps: 97000, Average Reward: -0.20085501182465054, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7466000\n",
      "Time steps: 98000, Average Reward: -0.20073110666463756, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7467000\n",
      "Time steps: 99000, Average Reward: -0.20113378684809877, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7468000\n",
      "Time steps: 100000, Average Reward: -0.2011239260915903, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7469000\n",
      "Time steps: 101000, Average Reward: -0.20113193640099916, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7470000\n",
      "Time steps: 102000, Average Reward: -0.2013384776603089, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7471000\n",
      "Time steps: 103000, Average Reward: -0.20137427154910714, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7472000\n",
      "Time steps: 104000, Average Reward: -0.2013843322397302, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7473000\n",
      "Time steps: 105000, Average Reward: -0.2013521523301276, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7474000\n",
      "Time steps: 106000, Average Reward: -0.20092432917152295, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7475000\n",
      "Time steps: 107000, Average Reward: -0.20124217590497492, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7476000\n",
      "Time steps: 108000, Average Reward: -0.20157301327509355, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7477000\n",
      "Time steps: 109000, Average Reward: -0.20190032577007716, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7478000\n",
      "Time steps: 110000, Average Reward: -0.20195977726111644, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7479000\n",
      "Time steps: 111000, Average Reward: -0.20191567933521629, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7480000\n",
      "Time steps: 112000, Average Reward: -0.20181246035998113, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7481000\n",
      "Time steps: 113000, Average Reward: -0.2017444294551962, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7482000\n",
      "Time steps: 114000, Average Reward: -0.20208249988879254, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7483000\n",
      "Time steps: 115000, Average Reward: -0.20240456228446105, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7484000\n",
      "Time steps: 116000, Average Reward: -0.202498265093595, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7485000\n",
      "Time steps: 117000, Average Reward: -0.20253363140752287, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7486000\n",
      "Time steps: 118000, Average Reward: -0.20238517568016834, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7487000\n",
      "Time steps: 119000, Average Reward: -0.202358439102347, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7488000\n",
      "Time steps: 120000, Average Reward: -0.20264589254998944, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7489000\n",
      "Time steps: 121000, Average Reward: -0.20282571108915695, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7490000\n",
      "Time steps: 122000, Average Reward: -0.20283798406389722, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7491000\n",
      "Time steps: 123000, Average Reward: -0.1918172690763032, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7492000\n",
      "Time steps: 124000, Average Reward: -0.19661823647293925, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7493000\n",
      "Time steps: 125000, Average Reward: -0.19746328437917143, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7494000\n",
      "Time steps: 126000, Average Reward: -0.19794794794795853, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7495000\n",
      "Time steps: 127000, Average Reward: -0.1980784627702333, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7496000\n",
      "Time steps: 128000, Average Reward: -0.1986157438292412, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7497000\n",
      "Time steps: 129000, Average Reward: -0.19890651801031697, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7498000\n",
      "Time steps: 130000, Average Reward: -0.19883691845925758, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7499000\n",
      "Time steps: 131000, Average Reward: -0.19920520231216868, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7500000\n",
      "Time steps: 132000, Average Reward: -0.19939975990399314, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7501000\n",
      "Time steps: 133000, Average Reward: -0.19969079665334658, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7502000\n",
      "Time steps: 134000, Average Reward: -0.19973324441480692, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7503000\n",
      "Time steps: 135000, Average Reward: -0.19995383194827995, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7504000\n",
      "Time steps: 136000, Average Reward: -0.19961417547868554, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7505000\n",
      "Time steps: 137000, Average Reward: -0.19970325420108792, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7506000\n",
      "Time steps: 138000, Average Reward: -0.19975931482866555, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7507000\n",
      "Time steps: 139000, Average Reward: -0.2000647211108008, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7508000\n",
      "Time steps: 140000, Average Reward: -0.20023616359185348, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7509000\n",
      "Time steps: 141000, Average Reward: -0.20040271636127727, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7510000\n",
      "Time steps: 142000, Average Reward: -0.20042758551703588, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7511000\n",
      "Time steps: 143000, Average Reward: -0.20052390931598738, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7512000\n",
      "Time steps: 144000, Average Reward: -0.20072740498264618, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7513000\n",
      "Time steps: 145000, Average Reward: -0.20089145938415812, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7514000\n",
      "Time steps: 146000, Average Reward: -0.2010522587096981, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7515000\n",
      "Time steps: 147000, Average Reward: -0.20069811169778076, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7516000\n",
      "Time steps: 148000, Average Reward: -0.2006827973533444, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7517000\n",
      "Time steps: 149000, Average Reward: -0.2010020002962421, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7518000\n",
      "Time steps: 150000, Average Reward: -0.20133947706805136, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7519000\n",
      "Time steps: 151000, Average Reward: -0.20153469444050826, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7520000\n",
      "Time steps: 152000, Average Reward: -0.20180024003189728, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7521000\n",
      "Time steps: 153000, Average Reward: -0.2035353535353552, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7522000\n",
      "Time steps: 154000, Average Reward: -0.1990635451504964, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7523000\n",
      "Time steps: 155000, Average Reward: -0.19729458917834902, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7524000\n",
      "Time steps: 156000, Average Reward: -0.19822603719599918, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7525000\n",
      "Time steps: 157000, Average Reward: -0.1995995550611918, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7526000\n",
      "Time steps: 158000, Average Reward: -0.1992538671519742, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7527000\n",
      "Time steps: 159000, Average Reward: -0.19932255581218541, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7528000\n",
      "Time steps: 160000, Average Reward: -0.19931287525019215, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7529000\n",
      "Time steps: 161000, Average Reward: -0.19955856386112258, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7530000\n",
      "Time steps: 162000, Average Reward: -0.19997893628228358, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7531000\n",
      "Time steps: 163000, Average Reward: -0.20017151024299856, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7532000\n",
      "Time steps: 164000, Average Reward: -0.2002783819051841, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7533000\n",
      "Time steps: 165000, Average Reward: -0.20034413765505513, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7534000\n",
      "Time steps: 166000, Average Reward: -0.20031863653202944, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7535000\n",
      "Time steps: 167000, Average Reward: -0.20010348395995659, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7536000\n",
      "Time steps: 168000, Average Reward: -0.2002613746369406, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7537000\n",
      "Time steps: 169000, Average Reward: -0.20068808729913407, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7538000\n",
      "Time steps: 170000, Average Reward: -0.20083738210911933, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7539000\n",
      "Time steps: 171000, Average Reward: -0.2007569613408422, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7540000\n",
      "Time steps: 172000, Average Reward: -0.20080020518074884, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7541000\n",
      "Time steps: 173000, Average Reward: -0.20073432544515832, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7542000\n",
      "Time steps: 174000, Average Reward: -0.2008746220050399, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7543000\n",
      "Time steps: 175000, Average Reward: -0.20096688152914632, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7544000\n",
      "Time steps: 176000, Average Reward: -0.20105341562025797, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7545000\n",
      "Time steps: 177000, Average Reward: -0.20108389467229135, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7546000\n",
      "Time steps: 178000, Average Reward: -0.20110609923504974, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7547000\n",
      "Time steps: 179000, Average Reward: -0.2007548594073375, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7548000\n",
      "Time steps: 180000, Average Reward: -0.20102382251308312, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7549000\n",
      "Time steps: 181000, Average Reward: -0.20135462361807455, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7550000\n",
      "Time steps: 182000, Average Reward: -0.20167486014568053, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7551000\n",
      "Time steps: 183000, Average Reward: -0.20178553861277784, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7552000\n",
      "Time steps: 184000, Average Reward: -0.19602615694164768, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7553000\n",
      "Time steps: 185000, Average Reward: -0.19952357071212967, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7554000\n",
      "Time steps: 186000, Average Reward: -0.1992484969939874, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7555000\n",
      "Time steps: 187000, Average Reward: -0.198910866299459, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7556000\n",
      "Time steps: 188000, Average Reward: -0.19843812575091677, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7557000\n",
      "Time steps: 189000, Average Reward: -0.19868201534870278, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7558000\n",
      "Time steps: 190000, Average Reward: -0.19926365456107673, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7559000\n",
      "Time steps: 191000, Average Reward: -0.19979359519642442, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7560000\n",
      "Time steps: 192000, Average Reward: -0.199966644429649, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7561000\n",
      "Time steps: 193000, Average Reward: -0.20008505103064936, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7562000\n",
      "Time steps: 194000, Average Reward: -0.20016827360380066, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7563000\n",
      "Time steps: 195000, Average Reward: -0.20022928130732096, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7564000\n",
      "Time steps: 196000, Average Reward: -0.2004425119285694, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7565000\n",
      "Time steps: 197000, Average Reward: -0.2001572102329332, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7566000\n",
      "Time steps: 198000, Average Reward: -0.2003568093903891, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7567000\n",
      "Time steps: 199000, Average Reward: -0.2001781918218907, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7568000\n",
      "Time steps: 200000, Average Reward: -0.2001323996704214, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7569000\n",
      "Time steps: 201000, Average Reward: -0.2002500833610631, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7570000\n",
      "Time steps: 202000, Average Reward: -0.2002843003052964, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7571000\n",
      "Time steps: 203000, Average Reward: -0.20040512153639206, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7572000\n",
      "Time steps: 204000, Average Reward: -0.20070020005708497, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7573000\n",
      "Time steps: 205000, Average Reward: -0.20096389924516952, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7574000\n",
      "Time steps: 206000, Average Reward: -0.201135078716103, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7575000\n",
      "Time steps: 207000, Average Reward: -0.20133783445852663, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7576000\n",
      "Time steps: 208000, Average Reward: -0.20109626310305262, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7577000\n",
      "Time steps: 209000, Average Reward: -0.20115411248740142, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7578000\n",
      "Time steps: 210000, Average Reward: -0.20148921982652904, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7579000\n",
      "Time steps: 211000, Average Reward: -0.20176644995345921, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7580000\n",
      "Time steps: 212000, Average Reward: -0.20190384217413776, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7581000\n",
      "Time steps: 213000, Average Reward: -0.20211709008457568, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7582000\n",
      "Time steps: 214000, Average Reward: -0.18904665314401786, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7583000\n",
      "Time steps: 215000, Average Reward: -0.1956798392498275, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7584000\n",
      "Time steps: 216000, Average Reward: -0.1998796630565507, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7585000\n",
      "Time steps: 217000, Average Reward: -0.19812482107071736, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7586000\n",
      "Time steps: 218000, Average Reward: -0.1988760293790462, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7587000\n",
      "Time steps: 219000, Average Reward: -0.19931731294376437, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7588000\n",
      "Time steps: 220000, Average Reward: -0.19959186816573896, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7589000\n",
      "Time steps: 221000, Average Reward: -0.20004003736823553, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7590000\n",
      "Time steps: 222000, Average Reward: -0.20016484163431475, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7591000\n",
      "Time steps: 223000, Average Reward: -0.2002159485937302, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7592000\n",
      "Time steps: 224000, Average Reward: -0.20035738111124252, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7593000\n",
      "Time steps: 225000, Average Reward: -0.20035238841034453, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7594000\n",
      "Time steps: 226000, Average Reward: -0.20033218602416655, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7595000\n",
      "Time steps: 227000, Average Reward: -0.20020010375748518, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7596000\n",
      "Time steps: 228000, Average Reward: -0.20014144759536162, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7597000\n",
      "Time steps: 229000, Average Reward: -0.20041631704636914, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7598000\n",
      "Time steps: 230000, Average Reward: -0.20048202267624005, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7599000\n",
      "Time steps: 231000, Average Reward: -0.20048876693529194, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7600000\n",
      "Time steps: 232000, Average Reward: -0.20061915319303994, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7601000\n",
      "Time steps: 233000, Average Reward: -0.200723336582297, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7602000\n",
      "Time steps: 234000, Average Reward: -0.200922266139586, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7603000\n",
      "Time steps: 235000, Average Reward: -0.20096310426642866, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7604000\n",
      "Time steps: 236000, Average Reward: -0.20070688658685645, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7605000\n",
      "Time steps: 237000, Average Reward: -0.20072574809509108, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7606000\n",
      "Time steps: 238000, Average Reward: -0.20087576042125468, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7607000\n",
      "Time steps: 239000, Average Reward: -0.2010473463302713, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7608000\n",
      "Time steps: 240000, Average Reward: -0.20135884950731958, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7609000\n",
      "Time steps: 241000, Average Reward: -0.2016986141926028, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7610000\n",
      "Time steps: 242000, Average Reward: -0.20034218930955527, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7611000\n",
      "Time steps: 243000, Average Reward: -0.19603973824288537, Best Reward: 0.95\n",
      "Model saved in: model-tmnt/best_model_7612000\n",
      "Time steps: 244000, Average Reward: -0.1920850687042165, Best Reward: 0.95\n",
      "Model saved in: model-tmnt/best_model_7613000\n",
      "Time steps: 245000, Average Reward: -0.19198588709677192, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7614000\n",
      "Time steps: 246000, Average Reward: -0.1959588353413588, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7615000\n",
      "Time steps: 247000, Average Reward: -0.19807820855614844, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7616000\n",
      "Time steps: 248000, Average Reward: -0.20035070140281458, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7617000\n",
      "Time steps: 249000, Average Reward: -0.20055088141027197, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7618000\n",
      "Time steps: 250000, Average Reward: -0.20130173564755066, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7619000\n",
      "Time steps: 251000, Average Reward: -0.20126573226547043, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7620000\n",
      "Time steps: 252000, Average Reward: -0.2015703203203473, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7621000\n",
      "Time steps: 253000, Average Reward: -0.20136788256230673, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7622000\n",
      "Time steps: 254000, Average Reward: -0.2014211369095585, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7623000\n",
      "Time steps: 255000, Average Reward: -0.20093249636100677, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7624000\n",
      "Time steps: 256000, Average Reward: -0.20090476984656558, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7625000\n",
      "Time steps: 257000, Average Reward: -0.20115840517240122, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7626000\n",
      "Time steps: 258000, Average Reward: -0.20106846769580258, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7627000\n",
      "Time steps: 259000, Average Reward: -0.20117395944500402, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7628000\n",
      "Time steps: 260000, Average Reward: -0.20139757378685108, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7629000\n",
      "Time steps: 261000, Average Reward: -0.20161546610164482, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7630000\n",
      "Time steps: 262000, Average Reward: -0.20158681636277098, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7631000\n",
      "Time steps: 263000, Average Reward: -0.20173494102773795, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7632000\n",
      "Time steps: 264000, Average Reward: -0.2017031812724405, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7633000\n",
      "Time steps: 265000, Average Reward: -0.20172446646334066, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7634000\n",
      "Time steps: 266000, Average Reward: -0.20177337213524293, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7635000\n",
      "Time steps: 267000, Average Reward: -0.2020159185802921, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7636000\n",
      "Time steps: 268000, Average Reward: -0.20235286762245283, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7637000\n",
      "Time steps: 269000, Average Reward: -0.20252880921885805, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7638000\n",
      "Time steps: 270000, Average Reward: -0.202754693751828, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7639000\n",
      "Time steps: 271000, Average Reward: -0.20083358032000195, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7640000\n",
      "Time steps: 272000, Average Reward: -0.19639361246062276, Best Reward: 0.95\n",
      "Model saved in: model-tmnt/best_model_7641000\n",
      "Time steps: 273000, Average Reward: -0.1922271661147216, Best Reward: 0.95\n",
      "Model saved in: model-tmnt/best_model_7642000\n",
      "Time steps: 274000, Average Reward: -0.18838523606289237, Best Reward: 0.95\n",
      "Model saved in: model-tmnt/best_model_7643000\n",
      "Time steps: 275000, Average Reward: -0.19042769857433975, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7644000\n",
      "Time steps: 276000, Average Reward: -0.19741784037558166, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7645000\n",
      "Time steps: 277000, Average Reward: -0.1984142914492095, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7646000\n",
      "Time steps: 278000, Average Reward: -0.2006731595531418, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7647000\n",
      "Time steps: 279000, Average Reward: -0.20000000000001283, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7648000\n",
      "Time steps: 280000, Average Reward: -0.19965397923877282, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7649000\n",
      "Time steps: 281000, Average Reward: -0.20054691110771022, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7650000\n",
      "Time steps: 282000, Average Reward: -0.20088105726874822, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7651000\n",
      "Time steps: 283000, Average Reward: -0.20080084795668823, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7652000\n",
      "Time steps: 284000, Average Reward: -0.20090085344013128, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7653000\n",
      "Time steps: 285000, Average Reward: -0.20091983604997288, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7654000\n",
      "Time steps: 286000, Average Reward: -0.20053085023062367, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7655000\n",
      "Time steps: 287000, Average Reward: -0.20062444960370898, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7656000\n",
      "Time steps: 288000, Average Reward: -0.2006596990586141, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7657000\n",
      "Time steps: 289000, Average Reward: -0.20041405010003419, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7658000\n",
      "Time steps: 290000, Average Reward: -0.2006197146729987, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7659000\n",
      "Time steps: 291000, Average Reward: -0.200530592444318, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7660000\n",
      "Time steps: 292000, Average Reward: -0.20063175347316162, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7661000\n",
      "Time steps: 293000, Average Reward: -0.20056513979767948, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7662000\n",
      "Time steps: 294000, Average Reward: -0.2006618439279007, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7663000\n",
      "Time steps: 295000, Average Reward: -0.2006783465911148, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7664000\n",
      "Time steps: 296000, Average Reward: -0.2006188637103136, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7665000\n",
      "Time steps: 297000, Average Reward: -0.20062914054502534, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7666000\n",
      "Time steps: 298000, Average Reward: -0.2005938444509804, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7667000\n",
      "Time steps: 299000, Average Reward: -0.20061859458567619, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7668000\n",
      "Time steps: 300000, Average Reward: -0.20045113961781036, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7669000\n",
      "Time steps: 301000, Average Reward: -0.20066815144756409, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7670000\n",
      "Time steps: 302000, Average Reward: -0.20092575752054243, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7671000\n",
      "Time steps: 303000, Average Reward: -0.2012284581094747, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7672000\n",
      "Time steps: 304000, Average Reward: -0.20142755416896188, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7673000\n",
      "Time steps: 305000, Average Reward: -0.2008658292609856, Best Reward: 0.85\n",
      "Model saved in: model-tmnt/best_model_7674000\n",
      "Time steps: 306000, Average Reward: -0.19853535353535137, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7675000\n",
      "Time steps: 307000, Average Reward: -0.19886934673366152, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7676000\n",
      "Time steps: 308000, Average Reward: -0.20088628762541738, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7677000\n",
      "Time steps: 309000, Average Reward: -0.20246867167920798, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7678000\n",
      "Time steps: 310000, Average Reward: -0.20253507014029676, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7679000\n",
      "Time steps: 311000, Average Reward: -0.20232053422372737, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7680000\n",
      "Time steps: 312000, Average Reward: -0.20227467811161268, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7681000\n",
      "Time steps: 313000, Average Reward: -0.20200876095121634, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7682000\n",
      "Time steps: 314000, Average Reward: -0.20197441601782698, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7683000\n",
      "Time steps: 315000, Average Reward: -0.201871871871903, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7684000\n",
      "Time steps: 316000, Average Reward: -0.20196997270247205, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7685000\n",
      "Time steps: 317000, Average Reward: -0.20144703919933288, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7686000\n",
      "Time steps: 318000, Average Reward: -0.20110084680522158, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7687000\n",
      "Time steps: 319000, Average Reward: -0.20115796997853172, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7688000\n",
      "Time steps: 320000, Average Reward: -0.20077384923278824, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7689000\n",
      "Time steps: 321000, Average Reward: -0.20118824265161456, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7690000\n",
      "Time steps: 322000, Average Reward: -0.20108593290165644, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7691000\n",
      "Time steps: 323000, Average Reward: -0.20123123957748557, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7692000\n",
      "Time steps: 324000, Average Reward: -0.20124012638224256, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7693000\n",
      "Time steps: 325000, Average Reward: -0.20125312656321198, Best Reward: 0.8499999999999999\n",
      "Model saved in: model-tmnt/best_model_7694000\n",
      "Time steps: 326000, Average Reward: -0.201250595521602, Best Reward: 0.8499999999999999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkServerProcess-174:\n",
      "Process ForkServerProcess-175:\n",
      "Process ForkServerProcess-180:\n",
      "Process ForkServerProcess-177:\n",
      "Process ForkServerProcess-173:\n",
      "Process ForkServerProcess-179:\n",
      "Process ForkServerProcess-176:\n",
      "Process ForkServerProcess-178:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/paulo101977/.local/lib/python3.12/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py\", line 33, in _worker\n",
      "    cmd, data = remote.recv()\n",
      "                ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 494\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone finded, starting from zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    479\u001b[0m     model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    480\u001b[0m                 env, \n\u001b[1;32m    481\u001b[0m                 verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m                 ent_coef\u001b[38;5;241m=\u001b[39mENT_COEF\n\u001b[1;32m    492\u001b[0m             )\n\u001b[0;32m--> 494\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOTAL_TIMESTEP_NUMB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmnt_final\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    497\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/common/on_policy_algorithm.py:247\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    244\u001b[0m             terminal_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict_values(terminal_obs)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         rewards[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m terminal_value\n\u001b[0;32m--> 247\u001b[0m \u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_episode_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/stable_baselines3/common/buffers.py:475\u001b[0m, in \u001b[0;36mRolloutBuffer.add\u001b[0;34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(reward)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_starts[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(episode_start)\n\u001b[0;32m--> 475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m log_prob\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import retro\n",
    "\n",
    "import gymnasium as gym\n",
    "import re\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecNormalize\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from gym.wrappers import GrayScaleObservation, FrameStack\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from gymnasium.wrappers import TimeLimit\n",
    "from stable_baselines3.common.atari_wrappers import ClipRewardEnv, WarpFrame\n",
    "\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from pytz import timezone\n",
    "from stable_baselines3.common.utils import get_schedule_fn\n",
    "\n",
    "def custom_schedule(progress_remaining):\n",
    "    #return 1e-4 + (3e-4 - 1e-4) * (1 - progress_remaining)\n",
    "    return 5e-5 + (2e-4 - 5e-5) * (1 - progress_remaining)\n",
    "\n",
    "# Model Param\n",
    "CHECK_FREQ_NUMB = 1000\n",
    "TOTAL_TIMESTEP_NUMB = 500_000_000\n",
    "LEARNING_RATE = custom_schedule # 0.00025 # 0.0002 # 0.0001 # 0.00025 # 0.0001\n",
    "GAE = 0.98 # 0.9 # 1.0 # 0.95 # 1.0\n",
    "ENT_COEF = 0.004 # 0.001 # 0.03 # 0.01 # 0.03 # 0.1 # 0.03 # 0.02 # 0.01 # 0.005 # 0.01\n",
    "N_STEPS = 2048 # 4096 # 512 # 2048 # 4096 # 2048 # 512\n",
    "GAMMA = 0.99 # 0.9\n",
    "BATCH_SIZE = 512 # 128 # 64\n",
    "CLIP_RANGE = 0.1 # 0.15 # 0.2 # 0.4 # 0.3\n",
    "N_EPOCHS = 10 # 6 # 10 # 15 # 10\n",
    "MAX_EPISODE=122000\n",
    "USE_CURRICULUM=False\n",
    "USE_CLIP_REWARD=False\n",
    "STATE=\"Leo.Level1.99Lives\"\n",
    "TENSORBOARD=\"./tensorboard-tmnt\"\n",
    "SAVE_DIR=\"./model-tmnt\"\n",
    "NUM_ENV = 8\n",
    "\n",
    "model = None\n",
    "\n",
    "# Test Param\n",
    "SAVE_FREQ=1000\n",
    "\n",
    "save_dir = Path(SAVE_DIR)\n",
    "GAME = \"TeenageMutantNinjaTurtlesIVTurtlesInTime-Snes\"\n",
    "states = retro.data.list_states(GAME)\n",
    "\n",
    "# print(retro.data.list_games())\n",
    "print(f\"States for {GAME}: {states}\")\n",
    "\n",
    "class Discretizer(gym.ActionWrapper):\n",
    "    \"\"\"\n",
    "    Wrap a gym environment and make it use discrete actions.\n",
    "\n",
    "    Args:\n",
    "        combos: ordered list of lists of valid button combinations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, combos):\n",
    "        super().__init__(env)\n",
    "        assert isinstance(env.action_space, gym.spaces.MultiBinary)\n",
    "        buttons = env.unwrapped.buttons\n",
    "        self._decode_discrete_action = []\n",
    "        for combo in combos:\n",
    "            arr = np.array([False] * env.action_space.n)\n",
    "            for button in combo:\n",
    "                arr[buttons.index(button)] = True\n",
    "            self._decode_discrete_action.append(arr)\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(len(self._decode_discrete_action))\n",
    "\n",
    "    def action(self, act):\n",
    "        return self._decode_discrete_action[act].copy()\n",
    "\n",
    "class MainDiscretizer(Discretizer):\n",
    "    \"\"\"\n",
    "    Use Sonic-specific discrete actions\n",
    "    based on https://github.com/openai/retro-baselines/blob/master/agents/sonic_util.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(\n",
    "            env=env,\n",
    "            combos=[\n",
    "                [\"LEFT\"],         # Move left\n",
    "                [\"RIGHT\"],        # Move right\n",
    "                [\"UP\"],           # Move up\n",
    "                [\"DOWN\"],         # Move down\n",
    "                [\"B\"],            # Attack\n",
    "                [\"Y\"],            # Special attack\n",
    "                [\"A\"],            # Jump attack\n",
    "                [\"X\"],            # Throw (in some cases)\n",
    "                [\"L\"],            # Block (if available)\n",
    "                [\"R\"],            # Not used, but reserved\n",
    "                \n",
    "                # Combined actions\n",
    "                [\"LEFT\", \"B\"],     # Move left and attack\n",
    "                [\"RIGHT\", \"B\"],    # Move right and attack\n",
    "                [\"UP\", \"B\"],       # Move up and attack\n",
    "                [\"DOWN\", \"B\"],     # Move down and attack\n",
    "                \n",
    "                [\"LEFT\", \"A\"],     # Move left and jump\n",
    "                [\"RIGHT\", \"A\"],    # Move right and jump\n",
    "                [\"UP\", \"A\"],       # Move up and jump\n",
    "                [\"DOWN\", \"A\"],     # Move down and jump\n",
    "\n",
    "                [\"LEFT\", \"A\", \"B\"],  # Jump left and attack\n",
    "                [\"RIGHT\", \"A\", \"B\"], # Jump right and attack\n",
    "\n",
    "                # [\"B\", \"Y\"],        # Attack + Special\n",
    "            ]\n",
    "        )\n",
    "\n",
    "class IgnorePauseActionWrapper(gym.ActionWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def action(self, act):\n",
    "        act[3] = 0\n",
    "        return act\n",
    "\n",
    "\n",
    "class ResetStateWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "        self.steps = 0\n",
    "        self.lose_lives = False\n",
    "        self.current_health = 16\n",
    "        self.current_lives = 3\n",
    "        self.acc = 0\n",
    "        self.score = 0\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        #self.x_last = self.env.unwrapped.data['x']\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "        self.current_lives = 2\n",
    "        self.current_health = 16\n",
    "\n",
    "        self.lose_lives = False\n",
    "        self.acc = 0\n",
    "        self.score = 0\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, trunc, info = self.env.step(action)\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        \n",
    "        reward = 0\n",
    "        # if reward > 0:\n",
    "        #     reward = 1 # new score reward\n",
    "\n",
    "\n",
    "        lives = info['lives']\n",
    "        health = info['health']\n",
    "        acc = info['acceleration_x_foward']\n",
    "        score = info['score']\n",
    "\n",
    "        if score > self.score:\n",
    "            self.score = score\n",
    "            reward = 0.5\n",
    "\n",
    "        # Existential to encourage agent move\n",
    "        reward -= 0.05\n",
    "\n",
    "        if self.acc != acc:\n",
    "            reward += 0.05\n",
    "            self.acc = acc\n",
    "\n",
    "        if health > 16:\n",
    "            health = 16\n",
    "\n",
    "        if health < self.current_health:\n",
    "            self.current_health = health\n",
    "            reward -=0.5\n",
    "\n",
    "        if health > self.current_health:\n",
    "            self.current_health = health\n",
    "            reward +=0.5\n",
    "\n",
    "        # Reach Max Trainning Step\n",
    "        if MAX_EPISODE > 0 and self.steps > MAX_EPISODE:\n",
    "            done = True\n",
    "            # reward -= 1\n",
    "\n",
    "        # lost lives\n",
    "        if lives < self.current_lives:\n",
    "            # reset health counter\n",
    "            self.current_health = 16\n",
    "            reward -= 1\n",
    "            self.current_lives = lives\n",
    "            # done = True\n",
    "\n",
    "        # win lives\n",
    "        if lives > self.current_lives:\n",
    "            reward += 1\n",
    "            self.current_lives =  lives\n",
    "\n",
    "        # lose\n",
    "        # if info['lives'] < 1:\n",
    "        #     reward -= 2\n",
    "        #     done = True\n",
    "\n",
    "        return obs, reward, done, trunc, info\n",
    "\n",
    "class RandomStateWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        self.env = env\n",
    "\n",
    "    def get_random_state(self):\n",
    "        \"\"\"Select a random state from folder STATE_PATH\"\"\"\n",
    "        STATE_PATH = \"./States\"\n",
    "        states = [f for f in os.listdir(STATE_PATH) if f.endswith(\".state\")]\n",
    "        if not states:\n",
    "            raise FileNotFoundError(\"File not found!\")\n",
    "        c = random.choice(states)\n",
    "\n",
    "        return os.path.abspath(\"./States/\" + c)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        state = self.get_random_state()\n",
    "        #print(f\"Loading state: {state}\")\n",
    "        self.env.load_state(state)\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, trunc, info = self.env.step(action)\n",
    "\n",
    "        if done or trunc:\n",
    "            self.reset()\n",
    "\n",
    "        return obs, reward, done, trunc, info\n",
    "\n",
    "\n",
    "class CurriculumWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, required_wins=20): #required_avg_reward=1.0):\n",
    "        super().__init__(env)\n",
    "        self.required_wins = required_wins\n",
    "        #self.required_avg_reward = required_avg_reward\n",
    "        self.current_phase = 1\n",
    "        self.total_wins = 0 \n",
    "        self.rewards_list = []\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "\n",
    "        self.rewards_list.append(reward)\n",
    "\n",
    "        could_to_next_stage = info[\"matches_won\"] / self.current_phase >= 2\n",
    "\n",
    "        if info[\"matches_won\"] % 2 == 0 and info[\"matches_won\"] > 0 and could_to_next_stage:\n",
    "            self.total_wins += 1\n",
    "\n",
    "\n",
    "        # avg_reward = np.mean(self.rewards_list[-self.required_wins:]) if len(self.rewards_list) >= self.required_wins else np.mean(self.rewards_list)\n",
    "        avg_reward = np.mean(self.rewards_list)\n",
    "\n",
    "        if could_to_next_stage and \\\n",
    "            ((info[\"matches_won\"] % 2 == 0  and info[\"matches_won\"] > 0) \\\n",
    "                 or (info[\"enemy_matches_won\"] % 2 == 0 and info[\"enemy_matches_won\"] > 0)) :\n",
    "            print(info)\n",
    "            print(f\" stage {self.current_phase}! ({self.total_wins} fights win, avg rewards: {avg_reward:.2f})\")\n",
    "            done = True\n",
    "        \n",
    "        if self.total_wins >= self.required_wins: #and avg_reward >= self.required_avg_reward:\n",
    "            self.current_phase += 1\n",
    "            print(f\" Going to next stage {self.current_phase}! ({self.total_wins} fights win, avg rewards: {avg_reward:.2f})\")\n",
    "            self.total_wins = 0\n",
    "            self.rewards_list = []\n",
    "\n",
    "        return obs, reward, done, truncated, info\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            observation, reward, terminated, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if terminated:\n",
    "                break\n",
    "        return observation, total_reward, terminated, trunk, info\n",
    "\n",
    "\n",
    "class GameNet(BaseFeaturesExtractor):\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim):\n",
    "        super(GameNet, self).__init__(observation_space, features_dim)\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.linear(self.cnn(observations))\n",
    "\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_freq=SAVE_FREQ,\n",
    "        self.save_path = save_path\n",
    "\n",
    "        self.best_reward = float('-inf')\n",
    "        self.episode_rewards = []\n",
    "        self.current_episode_reward = 0\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self):\n",
    "        reward = self.locals[\"rewards\"][0]\n",
    "        self.current_episode_reward = reward\n",
    "\n",
    "        done = self.locals[\"dones\"][0]\n",
    "\n",
    "        self.episode_rewards.append(self.current_episode_reward)\n",
    "\n",
    "        if done:\n",
    "            self.current_episode_reward = 0\n",
    "            self.episode_rewards = []\n",
    "            self.best_reward = float('-inf')\n",
    "        \n",
    "        if self.n_calls % self.check_freq == 0 and len(self.episode_rewards) > 0:\n",
    "            latest_model = get_latest_model(self.save_path)\n",
    "            next_save_step = (int(re.search(r\"best_model_(\\d+)\", str(latest_model)).group(1)) + self.check_freq) if latest_model else self.n_calls\n",
    "            model_path = self.save_path / f\"best_model_{next_save_step}\"\n",
    "            model.save(model_path)\n",
    "            print(f\"Model saved in: {model_path}\")\n",
    "\n",
    "            average_reward = sum(self.episode_rewards) / len(self.episode_rewards)\n",
    "            best_reward = max(self.episode_rewards)\n",
    "            sum_rewards = sum(self.episode_rewards)\n",
    "\n",
    "            self.best_reward = max(self.best_reward, best_reward)\n",
    "\n",
    "            self.logger.record(\"average_reward\", average_reward)\n",
    "            self.logger.record(\"best_reward\", self.best_reward)\n",
    "            self.logger.record(\"sum_rewards\", sum_rewards)\n",
    "\n",
    "            if USE_CURRICULUM:\n",
    "                self.logger.record(\"current_phase\", self.training_env.get_attr(\"current_phase\")[0])\n",
    "\n",
    "            print(f\"Time steps: {self.n_calls}, Average Reward: {average_reward}, Best Reward: {self.best_reward}\")\n",
    "\n",
    "\n",
    "        return True\n",
    "          \n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=GameNet,\n",
    "    features_extractor_kwargs=dict(features_dim=1024), # features_extractor_kwargs=dict(features_dim=512),\n",
    "    # net_arch=dict(\n",
    "    #     pi=[1024, 512, 256],  # Actor\n",
    "    #     vf=[1024, 1024, 512]  # Critic\n",
    "    # ) #\n",
    ")\n",
    "\n",
    "def get_latest_model(path):\n",
    "    models = list(path.glob(\"best_model_*\"))\n",
    "    if not models:\n",
    "        return None\n",
    "    model_numbers = [int(re.search(r\"best_model_(\\d+)\", str(m)).group(1)) for m in models]\n",
    "    latest_model = max(model_numbers)\n",
    "    return path / f\"best_model_{latest_model}\"\n",
    "\n",
    "def make_env():\n",
    "    def _init():\n",
    "        env = retro.make(\n",
    "            game=GAME, \n",
    "            #use_restricted_actions=retro.Actions.DISCRETE, \n",
    "            # render_mode=\"human\",\n",
    "            render_mode=None,\n",
    "            state=STATE\n",
    "        )\n",
    "\n",
    "        # env = DonkeyKongCustomActions(env)\n",
    "\n",
    "        env = MainDiscretizer(env)\n",
    "\n",
    "        # env = IgnorePauseActionWrapper(env)\n",
    "\n",
    "        # env = RandomStateWrapper(env)\n",
    "\n",
    "        env = ResetStateWrapper(env)\n",
    "        \n",
    "        env = SkipFrame(env, skip=4)\n",
    "        env = WarpFrame(env)\n",
    "\n",
    "        if USE_CURRICULUM:\n",
    "            env = CurriculumWrapper(env, required_wins=50) #, required_avg_reward=0.6)\n",
    "\n",
    "        if USE_CLIP_REWARD:\n",
    "            env = ClipRewardEnv(env)\n",
    "\n",
    "        #env = TimeLimit(env, max_episode_steps=MAX_EPISODE)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "env = SubprocVecEnv([make_env() for _ in range(NUM_ENV)])\n",
    "# env = DummyVecEnv([make_env()])\n",
    "# env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.0)\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "checkpoint_callback=TrainAndLoggingCallback(check_freq=CHECK_FREQ_NUMB, save_path=save_dir)\n",
    "\n",
    "\n",
    "latest_model_path = get_latest_model(save_dir)\n",
    "\n",
    "if latest_model_path:\n",
    "    print(f\"Loading existent model: {latest_model_path}\")\n",
    "    model = PPO.load(\n",
    "        str(latest_model_path), \n",
    "        env=env, \n",
    "        verbose=0, \n",
    "        tensorboard_log=TENSORBOARD, \n",
    "        learning_rate=LEARNING_RATE, \n",
    "        n_steps=N_STEPS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        n_epochs=N_EPOCHS, \n",
    "        gamma=GAMMA, \n",
    "        gae_lambda=GAE, \n",
    "        clip_range=CLIP_RANGE,\n",
    "        ent_coef=ENT_COEF,\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"None finded, starting from zero.\")\n",
    "    model = PPO('CnnPolicy', \n",
    "                env, \n",
    "                verbose=0, \n",
    "                policy_kwargs=policy_kwargs, \n",
    "                tensorboard_log=TENSORBOARD, \n",
    "                learning_rate=LEARNING_RATE, \n",
    "                n_steps=N_STEPS, \n",
    "                batch_size=BATCH_SIZE, \n",
    "                n_epochs=N_EPOCHS, \n",
    "                gamma=GAMMA, \n",
    "                gae_lambda=GAE, \n",
    "                clip_range=CLIP_RANGE,\n",
    "                ent_coef=ENT_COEF\n",
    "            )\n",
    "\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEP_NUMB, reset_num_timesteps=False, callback=checkpoint_callback)\n",
    "model.save(\"tmnt_final\")\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7188932c-fe0f-4d29-8e8e-45a876ca4df4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
